{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zscEUiFEUhcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "26510cf2-3c4a-47dc-a6b2-cd494ad240e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-16 20:48:27--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.194.101, 172.217.194.139, 172.217.194.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.194.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rqlicassqkst5t5sh9qbciietgass2dp/1684270050000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=8bbfed03-2c6b-43de-9ee2-c26a7e6c8ab6 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-16 20:48:32--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rqlicassqkst5t5sh9qbciietgass2dp/1684270050000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=8bbfed03-2c6b-43de-9ee2-c26a7e6c8ab6\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 172.253.118.132, 2404:6800:4003:c05::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|172.253.118.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  39.7MB/s    in 1.7s    \n",
            "\n",
            "2023-05-16 20:48:35 (39.7 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "2cb7f47f-3e4e-44b7-ae21-cfb0ec870c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "13545003-da6f-4ed9-e506-2136c22d9b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "ec3b66d8-aaa2-47c7-e6c0-48c9eefaae6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 15s 90ms/step - loss: 6.0042 - accuracy: 0.0242\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 2s 26ms/step - loss: 5.4432 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3736 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3206 - accuracy: 0.0368\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.2520 - accuracy: 0.0424\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 24ms/step - loss: 5.1897 - accuracy: 0.0404\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1371 - accuracy: 0.0404\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.0782 - accuracy: 0.0419\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.0162 - accuracy: 0.0399\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.9538 - accuracy: 0.0434\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.8833 - accuracy: 0.0454\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8132 - accuracy: 0.0545\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.7355 - accuracy: 0.0610\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6608 - accuracy: 0.0711\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5786 - accuracy: 0.0737\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5021 - accuracy: 0.0777\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 4.4232 - accuracy: 0.0807\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 4.3477 - accuracy: 0.0918\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 4.2687 - accuracy: 0.1004\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.2046 - accuracy: 0.1206\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1681 - accuracy: 0.1276\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.0711 - accuracy: 0.1498\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9838 - accuracy: 0.1675\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9075 - accuracy: 0.1862\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8363 - accuracy: 0.1998\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.7622 - accuracy: 0.2104\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.7061 - accuracy: 0.2482\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 3.6383 - accuracy: 0.2508\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 3.5730 - accuracy: 0.2699\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5095 - accuracy: 0.2800\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.4433 - accuracy: 0.2972\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3826 - accuracy: 0.3093\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3223 - accuracy: 0.3259\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2605 - accuracy: 0.3436\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2162 - accuracy: 0.3547\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1568 - accuracy: 0.3638\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0898 - accuracy: 0.3769\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0468 - accuracy: 0.3951\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9881 - accuracy: 0.3875\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9275 - accuracy: 0.4112\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8655 - accuracy: 0.4168\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8042 - accuracy: 0.4294\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7662 - accuracy: 0.4284\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.7312 - accuracy: 0.4450\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6867 - accuracy: 0.4516\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 2.6398 - accuracy: 0.4566\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5888 - accuracy: 0.4692\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5502 - accuracy: 0.4793\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 2.5066 - accuracy: 0.4899\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.4521 - accuracy: 0.4889\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.4198 - accuracy: 0.4950\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 2.3709 - accuracy: 0.5040\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 2.3331 - accuracy: 0.5045\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.2968 - accuracy: 0.5121\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2624 - accuracy: 0.5247\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2466 - accuracy: 0.5212\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.2180 - accuracy: 0.5303\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1534 - accuracy: 0.5449\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1104 - accuracy: 0.5570\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0675 - accuracy: 0.5671\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0299 - accuracy: 0.5711\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0196 - accuracy: 0.5752\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9976 - accuracy: 0.5797\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9471 - accuracy: 0.5853\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9148 - accuracy: 0.5949\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8828 - accuracy: 0.6060\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8436 - accuracy: 0.6125\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8151 - accuracy: 0.6150\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7856 - accuracy: 0.6231\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7542 - accuracy: 0.6261\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7282 - accuracy: 0.6403\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6920 - accuracy: 0.6448\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6602 - accuracy: 0.6524\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6418 - accuracy: 0.6519\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.6426 - accuracy: 0.6554\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.6266 - accuracy: 0.6509\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.5811 - accuracy: 0.6665\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.5483 - accuracy: 0.6741\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.5138 - accuracy: 0.6806\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4929 - accuracy: 0.6801\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4594 - accuracy: 0.6973\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4367 - accuracy: 0.6958\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4188 - accuracy: 0.7028\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3929 - accuracy: 0.7053\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3749 - accuracy: 0.7175\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3724 - accuracy: 0.7104\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3444 - accuracy: 0.7210\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3217 - accuracy: 0.7306\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2958 - accuracy: 0.7351\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2764 - accuracy: 0.7341\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2492 - accuracy: 0.7447\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2324 - accuracy: 0.7452\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2242 - accuracy: 0.7518\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2093 - accuracy: 0.7528\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1755 - accuracy: 0.7583\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1624 - accuracy: 0.7649\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1473 - accuracy: 0.7684\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1348 - accuracy: 0.7624\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1297 - accuracy: 0.7639\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.1003 - accuracy: 0.7709\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.0714 - accuracy: 0.7820\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.0536 - accuracy: 0.7825\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.0409 - accuracy: 0.7830\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0286 - accuracy: 0.7876\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0184 - accuracy: 0.7866\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0173 - accuracy: 0.7886\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9933 - accuracy: 0.7881\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9746 - accuracy: 0.7936\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9653 - accuracy: 0.7987\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9626 - accuracy: 0.7972\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9553 - accuracy: 0.7941\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9399 - accuracy: 0.7987\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9260 - accuracy: 0.8047\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9155 - accuracy: 0.8027\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9075 - accuracy: 0.8103\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8788 - accuracy: 0.8128\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8729 - accuracy: 0.8138\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8699 - accuracy: 0.8148\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8567 - accuracy: 0.8138\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8605 - accuracy: 0.8174\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8364 - accuracy: 0.8179\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8280 - accuracy: 0.8179\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8207 - accuracy: 0.8204\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8037 - accuracy: 0.8244\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7928 - accuracy: 0.8219\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.7932 - accuracy: 0.8244\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8015 - accuracy: 0.8204\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7700 - accuracy: 0.8285\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7575 - accuracy: 0.8330\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7415 - accuracy: 0.8355\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7238 - accuracy: 0.8391\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7121 - accuracy: 0.8426\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7081 - accuracy: 0.8451\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6960 - accuracy: 0.8451\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6866 - accuracy: 0.8476\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6788 - accuracy: 0.8446\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6733 - accuracy: 0.8517\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6656 - accuracy: 0.8532\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6778 - accuracy: 0.8486\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6807 - accuracy: 0.8441\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6597 - accuracy: 0.8502\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6508 - accuracy: 0.8491\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6338 - accuracy: 0.8557\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6348 - accuracy: 0.8542\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6353 - accuracy: 0.8557\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6206 - accuracy: 0.8562\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6193 - accuracy: 0.8547\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6364 - accuracy: 0.8522\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6133 - accuracy: 0.8592\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5973 - accuracy: 0.8607\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.5779 - accuracy: 0.8648\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.5708 - accuracy: 0.8688\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.5715 - accuracy: 0.8668\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5717 - accuracy: 0.8613\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5676 - accuracy: 0.8638\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5606 - accuracy: 0.8683\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5585 - accuracy: 0.8729\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5470 - accuracy: 0.8724\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5382 - accuracy: 0.8724\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5384 - accuracy: 0.8693\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5242 - accuracy: 0.8744\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.8769\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5068 - accuracy: 0.8759\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.8769\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5010 - accuracy: 0.8804\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5621 - accuracy: 0.8592\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6264 - accuracy: 0.8340\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5501 - accuracy: 0.8623\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5271 - accuracy: 0.8683\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5368 - accuracy: 0.8658\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5471 - accuracy: 0.8582\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5909 - accuracy: 0.8491\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5173 - accuracy: 0.8683\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5621 - accuracy: 0.8527\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5302 - accuracy: 0.8643\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.4920 - accuracy: 0.8749\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.4779 - accuracy: 0.8799\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.4674 - accuracy: 0.8850\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4545 - accuracy: 0.8850\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4457 - accuracy: 0.8890\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4408 - accuracy: 0.8880\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4351 - accuracy: 0.8910\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4311 - accuracy: 0.8925\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4249 - accuracy: 0.8935\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4212 - accuracy: 0.8900\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4163 - accuracy: 0.8925\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4123 - accuracy: 0.8930\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4084 - accuracy: 0.8946\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4059 - accuracy: 0.8905\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4016 - accuracy: 0.8961\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4021 - accuracy: 0.8920\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3951 - accuracy: 0.8976\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3912 - accuracy: 0.8971\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3890 - accuracy: 0.8976\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3866 - accuracy: 0.8981\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3810 - accuracy: 0.8971\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3792 - accuracy: 0.8981\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3764 - accuracy: 0.9001\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3820 - accuracy: 0.8976\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3729 - accuracy: 0.8946\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "565e2291-0df9-4f4e-a086-34623972c6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "a64ce2dd-bca7-44af-d9d6-f0d2f069051e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 635ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "im feeling chills me the eyes will you final hour pain leave me cause be me girl for yourself a break together of give yourself a break a break alone of wait wait learn heel heel heel heel heel heel heel heel heel heel heel heel heel heel fine heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel heel\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}